{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankiitg/LLM_Prod/blob/main/VLLM_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "996401ee-b419-4220-8bd0-399e1d327721",
      "metadata": {
        "id": "996401ee-b419-4220-8bd0-399e1d327721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae6d815-e34c-4a93-baca-56eb624d37d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate sentencepiece torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5389df77-2c98-4b99-8014-58a437055b3f",
      "metadata": {
        "id": "5389df77-2c98-4b99-8014-58a437055b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d781d860-ae44-4a66-e92c-d91d99a9eb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.6/734.6 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.6/950.6 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8ac227-510d-4190-8a66-73e3582092cc",
      "metadata": {
        "id": "1f8ac227-510d-4190-8a66-73e3582092cc"
      },
      "outputs": [],
      "source": [
        "import platform, torch, vllm\n",
        "print(\"vLLM:\", vllm.__version__)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47db9a2-f524-4560-88bf-73aa98489bca",
      "metadata": {
        "id": "f47db9a2-f524-4560-88bf-73aa98489bca"
      },
      "outputs": [],
      "source": [
        "!pip install -q matplotlib\n",
        "import time, itertools, os, json, math, random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80aac60b-ecab-46c5-a861-dccec11f0d39",
      "metadata": {
        "id": "80aac60b-ecab-46c5-a861-dccec11f0d39"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# --- Setup ---\n",
        "MODEL_NAME = \"Qwen/Qwen1.5-1.8B\"\n",
        "tokenizer_hf = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "if tokenizer_hf.pad_token is None:\n",
        "    tokenizer_hf.pad_token = tokenizer_hf.eos_token\n",
        "\n",
        "tokenizer_hf.padding_side = 'left'\n",
        "\n",
        "model_hf = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "pipe_hf = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_hf,\n",
        "    tokenizer=tokenizer_hf,\n",
        "    max_new_tokens=10,\n",
        "    batch_size=4,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# --- Prompt ---\n",
        "SYSTEM_PROMPT = \"\"\"Task: Determine if code needs migration from Python to C++.\n",
        "Rules:\n",
        "- If the code is Python: answer \"Yes\"\n",
        "- If the code is C++ or other language: answer \"No\"\n",
        "- Answer with only one word: \"Yes\" or \"No\"\n",
        "\n",
        "Examples:\n",
        "Code: def func(): pass\n",
        "Answer: Yes\n",
        "\n",
        "Code: #include <iostream>\n",
        "Answer: No\n",
        "\"\"\"\n",
        "\n",
        "# --- Classification Function ---\n",
        "def classify_code(snippets):\n",
        "    prompts = [\n",
        "        f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "Code: {code}\n",
        "Answer:\"\"\"\n",
        "        for code in snippets\n",
        "    ]\n",
        "\n",
        "    outputs = pipe_hf(prompts, return_full_text=False)\n",
        "\n",
        "    def extract_answer(text):\n",
        "        response = text[0][\"generated_text\"].strip()\n",
        "        # Take first line and first word\n",
        "        first_word = response.split('\\n')[0].split()[0] if response.split() else \"\"\n",
        "\n",
        "        if \"yes\" in first_word.lower():\n",
        "            return \"Yes\"\n",
        "        elif \"no\" in first_word.lower():\n",
        "            return \"No\"\n",
        "        return f\"Unclear: {first_word}\"\n",
        "\n",
        "    return [extract_answer(out) for out in outputs]\n",
        "\n",
        "# --- Test Snippets ---\n",
        "python_snippet = \"\"\"def add(a, b):\n",
        "    return a + b\n",
        "if __name__ == \"__main__\":\n",
        "    print(add(2, 3))\"\"\"\n",
        "\n",
        "cpp_snippet = \"\"\"#include <iostream>\n",
        "int add(int a, int b) {\n",
        "    return a + b;\n",
        "}\n",
        "int main() {\n",
        "    std::cout << add(2, 3) << std::endl;\n",
        "    return 0;\n",
        "}\"\"\"\n",
        "\n",
        "javascript_snippet = \"\"\"function add(a, b) {\n",
        "    return a + b;\n",
        "}\n",
        "console.log(add(2, 3));\"\"\"\n",
        "\n",
        "# --- Run Classification ---\n",
        "snippets = [python_snippet, cpp_snippet, javascript_snippet]\n",
        "labels = classify_code(snippets)\n",
        "\n",
        "for i, (code, label) in enumerate(zip(snippets, labels), 1):\n",
        "    print(f\"--- Snippet {i} ---\")\n",
        "    lang = \"Python\" if \"def \" in code else \"C++\" if \"#include\" in code else \"JavaScript\"\n",
        "    print(f\"Language: {lang}\")\n",
        "    print(f\"First few lines: {code.split(chr(10))[0]}\")\n",
        "    print(f\"→ Needs migration to C++: {label}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch\n",
        "\n",
        "del pipe_hf, model_hf, tokenizer_hf      # remove the HF objects\n",
        "gc.collect()                             # reclaim Python refs\n",
        "torch.cuda.empty_cache()                 # actually free GPU memory\n"
      ],
      "metadata": {
        "id": "kMH8MZ_9BpaM"
      },
      "id": "kMH8MZ_9BpaM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb828520-a6af-4869-b6e3-025d9e95b77e",
      "metadata": {
        "id": "fb828520-a6af-4869-b6e3-025d9e95b77e"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "\n",
        "# --- Model Configuration ---\n",
        "MODEL_NAME = \"Qwen/Qwen1.5-1.8B\"\n",
        "\n",
        "# --- Initialize vLLM model with fixed configuration ---\n",
        "llm_vllm = LLM(\n",
        "    model=MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    dtype=\"half\",\n",
        "    gpu_memory_utilization=0.85,  # Increased from 0.2\n",
        "    max_model_len=1024,          # Reduced from default 32768\n",
        "\n",
        ")\n",
        "\n",
        "# --- Sampling parameters ---\n",
        "sampling = SamplingParams(\n",
        "    max_tokens=10,\n",
        "    temperature=0.0,\n",
        "    stop=[\"\\n\", \"Code:\", \"###\", \"Task:\", \"Rules:\"]  # More stop tokens\n",
        ")\n",
        "\n",
        "# --- System Prompt ---\n",
        "SYSTEM_PROMPT = \"\"\"Task: Determine if code needs migration from Python to C++.\n",
        "Rules:\n",
        "- If the code is Python: answer \"Yes\"\n",
        "- If the code is C++ or other language: answer \"No\"\n",
        "- Answer with only one word: \"Yes\" or \"No\"\n",
        "\n",
        "Examples:\n",
        "Code: def func(): pass\n",
        "Answer: Yes\n",
        "\n",
        "Code: #include <iostream>\n",
        "Answer: No\n",
        "\"\"\"\n",
        "\n",
        "# --- vLLM Classification Function ---\n",
        "def classify_vllm(snippets):\n",
        "    prompts = [\n",
        "        f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "Code: {code}\n",
        "Answer:\"\"\"\n",
        "        for code in snippets\n",
        "    ]\n",
        "\n",
        "    outputs = llm_vllm.generate(prompts, sampling)\n",
        "\n",
        "    def extract_answer(output):\n",
        "        response = output.outputs[0].text.strip()\n",
        "        print(f\"vLLM raw output: '{response}'\")  # Debug print\n",
        "\n",
        "        # Clean up the response\n",
        "        first_word = response.split()[0] if response.split() else \"\"\n",
        "\n",
        "        if \"yes\" in first_word.lower():\n",
        "            return \"Yes\"\n",
        "        elif \"no\" in first_word.lower():\n",
        "            return \"No\"\n",
        "        return f\"Unclear: {first_word}\"\n",
        "\n",
        "    return [extract_answer(o) for o in outputs]\n",
        "\n",
        "# --- Test Snippets ---\n",
        "python_snippet = \"\"\"def add(a, b):\n",
        "    return a + b\n",
        "if __name__ == \"__main__\":\n",
        "    print(add(2, 3))\"\"\"\n",
        "\n",
        "cpp_snippet = \"\"\"#include <iostream>\n",
        "int add(int a, int b) {\n",
        "    return a + b;\n",
        "}\n",
        "int main() {\n",
        "    std::cout << add(2, 3) << std::endl;\n",
        "    return 0;\n",
        "}\"\"\"\n",
        "\n",
        "javascript_snippet = \"\"\"function add(a, b) {\n",
        "    return a + b;\n",
        "}\n",
        "console.log(add(2, 3));\"\"\"\n",
        "\n",
        "# --- Test vLLM Classification ---\n",
        "print(\"=== Testing vLLM Classification ===\")\n",
        "test_snippets = [python_snippet, cpp_snippet, javascript_snippet]\n",
        "labels = classify_vllm(test_snippets)\n",
        "\n",
        "for i, (code, label) in enumerate(zip(test_snippets, labels), 1):\n",
        "    print(f\"\\n--- Snippet {i} ---\")\n",
        "    lang = \"Python\" if \"def \" in code else \"C++\" if \"#include\" in code else \"JavaScript\"\n",
        "    print(f\"Language: {lang}\")\n",
        "    print(f\"First line: {code.split(chr(10))[0]}\")\n",
        "    print(f\"→ Needs migration to C++: {label}\")\n",
        "\n",
        "print(\"\\n=== vLLM Classification Ready for Benchmarking ===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641b4733-e6a7-4197-a49d-6520de799ede",
      "metadata": {
        "id": "641b4733-e6a7-4197-a49d-6520de799ede"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9b38b5-ca2b-4dfa-8d1d-80365e3aef31",
      "metadata": {
        "id": "3c9b38b5-ca2b-4dfa-8d1d-80365e3aef31"
      },
      "outputs": [],
      "source": [
        "%env TRANSFORMERS_VERBOSITY=error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8679d08-5243-4401-af47-062a6a003e8c",
      "metadata": {
        "id": "b8679d08-5243-4401-af47-062a6a003e8c"
      },
      "outputs": [],
      "source": [
        "# --- Benchmark utilities ---\n",
        "import time, math\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def gpu_sync():\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Your three base snippets (already defined above, repeated here for clarity).\n",
        "python_snippet = \"\"\"def add(a, b):\n",
        "    return a + b\n",
        "if __name__ == \"__main__\":\n",
        "    print(add(2, 3))\"\"\"\n",
        "\n",
        "cpp_snippet = \"\"\"#include <iostream>\n",
        "int add(int a, int b) {\n",
        "    return a + b;\n",
        "}\n",
        "int main() {\n",
        "    std::cout << add(2, 3) << std::endl;\n",
        "    return 0;\n",
        "}\"\"\"\n",
        "\n",
        "javascript_snippet = \"\"\"function add(a, b) {\n",
        "    return a + b;\n",
        "}\n",
        "console.log(add(2, 3));\"\"\"\n",
        "\n",
        "BASE_SNIPPETS = [python_snippet, cpp_snippet, javascript_snippet]\n",
        "\n",
        "def build_prompts(snippets, system_prompt: str, n: int):\n",
        "    \"\"\"Replicate the 3 base snippets to reach total size n.\"\"\"\n",
        "    unit = [\n",
        "        f\"\"\"{system_prompt}\n",
        "\n",
        "Code: {code}\n",
        "Answer:\"\"\"\n",
        "        for code in snippets\n",
        "    ]\n",
        "    reps = math.ceil(n / len(unit))\n",
        "    prompts = (unit * reps)[:n]\n",
        "    return prompts\n",
        "\n",
        "def benchmark_hf(prompts, pipe, tokenizer, batch_size: int = 4, warmup: bool = True):\n",
        "    \"\"\"Return (elapsed_s, samples_per_s, tokens_per_s).\"\"\"\n",
        "    # Warm-up (compile kernels / fill caches)\n",
        "    if warmup and len(prompts) > 0:\n",
        "        _ = pipe(prompts[:min(2, len(prompts))], return_full_text=False, batch_size=batch_size, num_return_sequences=1)\n",
        "        try:\n",
        "            import torch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    outs = pipe(prompts, return_full_text=False, batch_size=batch_size, num_return_sequences=1)\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "    except Exception:\n",
        "        pass\n",
        "    elapsed = time.perf_counter() - t0\n",
        "\n",
        "    # outs is List[List[{\"generated_text\": str}]]\n",
        "    gen_texts = []\n",
        "    for item in outs:\n",
        "        # single prompt ⇒ item is a list; grab the first generated sequence\n",
        "        first = item[0] if isinstance(item, list) else item\n",
        "        gen_texts.append(first[\"generated_text\"])\n",
        "\n",
        "    # Count generated tokens (using the same tokenizer)\n",
        "    gen_tokens = 0\n",
        "    for txt in gen_texts:\n",
        "        gen_tokens += len(tokenizer(txt, add_special_tokens=False).input_ids)\n",
        "\n",
        "    n = len(prompts)\n",
        "    sps = n / elapsed if elapsed > 0 else float(\"inf\")\n",
        "    tps = gen_tokens / elapsed if elapsed > 0 else float(\"inf\")\n",
        "    return elapsed, sps, tps\n",
        "\n",
        "\n",
        "def benchmark_vllm(prompts, llm, sampling, warmup: bool = True):\n",
        "    \"\"\"Return (elapsed_s, samples_per_s, tokens_per_s).\"\"\"\n",
        "    if warmup and len(prompts) > 0:\n",
        "        _ = llm.generate(prompts[:min(2, len(prompts))], sampling)\n",
        "        try:\n",
        "            import torch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    outs = llm.generate(prompts, sampling)\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "    except Exception:\n",
        "        pass\n",
        "    elapsed = time.perf_counter() - t0\n",
        "\n",
        "    # Count tokens via the same HF tokenizer for parity with HF pipeline\n",
        "    gen_tokens = 0\n",
        "    for o in outs:\n",
        "        if o.outputs:\n",
        "            txt = o.outputs[0].text\n",
        "            gen_tokens += len(tokenizer_hf(txt, add_special_tokens=False).input_ids)\n",
        "\n",
        "    n = len(prompts)\n",
        "    sps = n / elapsed if elapsed > 0 else float(\"inf\")\n",
        "    tps = gen_tokens / elapsed if elapsed > 0 else float(\"inf\")\n",
        "    return elapsed, sps, tps\n",
        "\n",
        "\n",
        "# --- Run the benchmark ---\n",
        "dataset_sizes = [3, 30, 300]\n",
        "rows = []\n",
        "\n",
        "# HF pipeline batch size (you initialized pipe_hf with batch_size=4; you can override per-call)\n",
        "HF_BATCH_SIZE = 4\n",
        "\n",
        "for n in dataset_sizes:\n",
        "    prompts = build_prompts(BASE_SNIPPETS, SYSTEM_PROMPT, n)\n",
        "\n",
        "    # HF\n",
        "    hf_elapsed, hf_sps, hf_tps = benchmark_hf(prompts, pipe_hf, tokenizer_hf, batch_size=HF_BATCH_SIZE)\n",
        "    rows.append({\n",
        "        \"engine\": \"HF/transformers\",\n",
        "        \"dataset_size\": n,\n",
        "        \"elapsed_s\": hf_elapsed,\n",
        "        \"samples_per_s\": hf_sps,\n",
        "        \"tokens_per_s\": hf_tps\n",
        "    })\n",
        "\n",
        "    # vLLM\n",
        "    v_elapsed, v_sps, v_tps = benchmark_vllm(prompts, llm_vllm, sampling)\n",
        "    rows.append({\n",
        "        \"engine\": \"vLLM\",\n",
        "        \"dataset_size\": n,\n",
        "        \"elapsed_s\": v_elapsed,\n",
        "        \"samples_per_s\": v_sps,\n",
        "        \"tokens_per_s\": v_tps\n",
        "    })\n",
        "\n",
        "# Tabulate\n",
        "df = pd.DataFrame(rows).sort_values([\"dataset_size\", \"engine\"]).reset_index(drop=True)\n",
        "print(df)\n",
        "\n",
        "# Save CSV\n",
        "df.to_csv(\"inference_benchmark.csv\", index=False)\n",
        "\n",
        "# --- Plot: dataset size vs samples/sec ---\n",
        "plt.figure()\n",
        "for eng in df[\"engine\"].unique():\n",
        "    sub = df[df[\"engine\"] == eng]\n",
        "    plt.plot(sub[\"dataset_size\"], sub[\"samples_per_s\"], marker=\"o\", label=eng)\n",
        "\n",
        "plt.xlabel(\"Dataset size (number of prompts)\")\n",
        "plt.ylabel(\"Throughput (samples/sec)\")\n",
        "plt.title(\"Inference Throughput vs Dataset Size (HF vs vLLM)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"inference_benchmark.png\", dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" - inference_benchmark.csv\")\n",
        "print(\" - inference_benchmark.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7acba5ba-7d9a-489a-9cb4-f0f82a4c1f95",
      "metadata": {
        "id": "7acba5ba-7d9a-489a-9cb4-f0f82a4c1f95"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}